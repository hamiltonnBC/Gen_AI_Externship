{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c7d2897",
   "metadata": {},
   "source": [
    "# Transformer Text Generation\n",
    "\n",
    "In this notebook, we will explore how transformer models (like GPT-2) can generate text based on a given prompt. We will experiment with generating text by adjusting parameters like temperature and sequence length.\n",
    "\n",
    "## Instructions\n",
    "1. Change the prompt below to experiment with different types of text generation.\n",
    "2. Adjust the `max_length` and `temperature` parameters to see how they affect the output.\n",
    "3. Generate at least 3 samples with different prompts and compare the results.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "6dbce095",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T18:23:52.307609Z",
     "start_time": "2025-05-25T18:22:44.653415Z"
    }
   },
   "source": [
    "from transformers import pipeline\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Clear the transformers cache\n",
    "cache_dir = os.path.expanduser(\"~/.cache/huggingface/transformers\")\n",
    "if os.path.exists(cache_dir):\n",
    "    shutil.rmtree(cache_dir)\n",
    "\n",
    "# Load GPT-2 text generation model\n",
    "generator = pipeline('text-generation', model='gpt2')\n",
    "\n",
    "\n",
    "# Set your prompt\n",
    "prompt = 'In the future, education will'\n",
    "\n",
    "# Generate text\n",
    "result = generator(prompt, max_length=50, temperature=0.7)\n",
    "print(result[0]['generated_text'])"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hamiltonn\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\hamiltonn\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "C:\\Users\\hamiltonn\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\hamiltonn\\.cache\\huggingface\\hub\\models--gpt2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\hamiltonn\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tf_keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the weights of TFGPT2LMHeadModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n",
      "Device set to use 0\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the future, education will include the creation of more than 1,700 colleges, universities and training centers in the United States. These facilities will be staffed by more than 4,000 teachers and students.\n",
      "\n",
      "The administration is expected to announce details\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "c69a033d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T20:21:39.896090Z",
     "start_time": "2025-05-25T20:19:38.258636Z"
    }
   },
   "source": [
    "# Experiment with different prompts\n",
    "print('prompt 1, temperature 0.1')\n",
    "prompt = 'The impact of AI on the future of work'\n",
    "result = generator(prompt, max_length=100, temperature=0.1)\n",
    "print(result[0]['generated_text'])\n",
    "\n",
    "print('prompt 2, temperature 0.1')\n",
    "prompt = 'The impact of AI on the future of work'\n",
    "result = generator(prompt, max_length=100, temperature=0.1)\n",
    "print(result[0]['generated_text'])\n",
    "\n",
    "print('prompt 3, temperature 0.7')\n",
    "prompt = 'The impact of AI on the future of work'\n",
    "result = generator(prompt, max_length=100, temperature=0.7)\n",
    "print(result[0]['generated_text'])\n",
    "\n",
    "print('prompt 4, temperature 0.7')\n",
    "prompt = 'The impact of AI on the future of work'\n",
    "result = generator(prompt, max_length=100, temperature=0.7)\n",
    "print(result[0]['generated_text'])\n",
    "\n",
    "print('prompt 5, temperature 2.0')\n",
    "prompt = 'The impact of AI on the future of work'\n",
    "result = generator(prompt, max_length=100, temperature=2.0)\n",
    "print(result[0]['generated_text'])\n",
    "\n",
    "print('prompt 6, temperature 2.0')\n",
    "prompt = 'The impact of AI on the future of work'\n",
    "result = generator(prompt, max_length=50, temperature=2.0)\n",
    "print(result[0]['generated_text'])\n",
    "\n",
    "\n",
    "# prompt = 'Once upon a time, there was a kingdom'\n",
    "# result = generator(prompt, max_length=100, temperature=0.6)\n",
    "# print(result[0]['generated_text'])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt 1, temperature 0.1\n",
      "The impact of AI on the future of work is clear.\n",
      "\n",
      "\"The future of work is not just about the future of work, but also about the future of work itself,\" says Dr. David H. Hirsch, a professor of psychology at the University of California, Berkeley. \"We're seeing a shift in the way people think about work. We're seeing a shift in the way people think about work. We're seeing a shift in the way people think about work.\"\n",
      "\n",
      "\n",
      "prompt 2, temperature 0.1\n",
      "The impact of AI on the future of work is clear.\n",
      "\n",
      "The AI revolution is already happening. The world is changing, and the future is changing.\n",
      "\n",
      "The future is changing.\n",
      "\n",
      "The future is changing.\n",
      "\n",
      "The future is changing.\n",
      "\n",
      "The future is changing.\n",
      "\n",
      "The future is changing.\n",
      "\n",
      "The future is changing.\n",
      "\n",
      "The future is changing.\n",
      "\n",
      "The future is changing.\n",
      "\n",
      "The future is changing.\n",
      "\n",
      "The future is\n",
      "prompt 3, temperature 0.7\n",
      "The impact of AI on the future of work and education is being felt in countries like Taiwan, where the government has already been forced to put up stiff resistance to the idea of the AI being used for research and development.\n",
      "\n",
      "The AI is being used to train workers to teach them how to work in the field of robotics, for instance. This has led to a huge increase in the number of people working on a robot.\n",
      "\n",
      "\"In Taiwan, it is common for students to get trained\n",
      "prompt 4, temperature 0.7\n",
      "The impact of AI on the future of work is still quite small, but it is estimated to be less than 1% of all worker productivity.\n",
      "\n",
      "But the future of work is going to be a very different story.\n",
      "\n",
      "There is a lot of uncertainty about what AI will do for the future and it appears that the future will involve the creation of new jobs.\n",
      "\n",
      "In recent years, many people have complained about automation and automation will have an important impact on the jobs of workers.\n",
      "prompt 5, temperature 2.0\n",
      "The impact of AI on the future of work depends less the number scientists find themselves with and over, over or about:\n",
      "\n",
      "You might expect that humans and machine science would work quite as well - that each computer should work more often with its current toolkit - unless the human is in danger of working too deeply in that area - this is something researchers have always been working against.\n",
      "\n",
      "Of course we've always heard an artificiality mantra, whether the machines \"like me\" or simply go\n",
      "prompt 6, temperature 2.0\n",
      "The impact of AI on the future of work remains unguarded: an independent panel studying computer technology has calculated AI may increase productivity just 4% or 20% in three decades\n",
      "\n",
      "Researchers have warned about a decline in new ideas if artificial intelligence is\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "id": "de0d0d32",
   "metadata": {},
   "source": [
    "## Reflection\n",
    "\n",
    "Now that you have experimented with text generation, write a brief report on your observations.\n",
    "\n",
    "1. What patterns did you notice in the generated text?\n",
    "2. How did changing the temperature affect the creativity and coherence of the text?\n",
    "3. What types of prompts yielded the most coherent results?\n",
    "4. What are the limitations of GPT-2 based on your experimentation?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
